{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import glob \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The glob module helps to take multiple inputs from a directory \n",
    "path_short=glob.glob(\"short_length/*.jpg\") \n",
    "# it takes all the images as an input from the short_length directory. \n",
    "#You should change the directory while working with your own pc.\n",
    "path_long=glob.glob(\"long_length/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_short={} # dictionary for saving multiple images within it's key. \n",
    "dic_long={}\n",
    "for img in path_short:\n",
    "    dic_short[img]=cv2.imread(img)\n",
    "    \n",
    "for img in path_long:\n",
    "    dic_long[img]=cv2.imread(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESIRED_HEIGHT = 480\n",
    "DESIRED_WIDTH = 480\n",
    "\n",
    "#function fore resizing the images \n",
    "def resize_and_show(name, image):\n",
    "    h, w = image.shape[:2]\n",
    "    if h < w:\n",
    "        img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
    "    else:\n",
    "        img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
    "    cv2.imshow(name,img)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "#preview the images\n",
    "# if you are in jupyter notebook the images wil open in a new window \n",
    "# image frame name and image name is same \n",
    "for name, image in dic_short.items():  \n",
    "    resize_and_show(name, image) \n",
    "\n",
    "for name, image in dic_long.items():\n",
    "    resize_and_show(name,image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FaceDetection in module mediapipe.python.solutions.face_detection:\n",
      "\n",
      "class FaceDetection(mediapipe.python.solution_base.SolutionBase)\n",
      " |  FaceDetection(min_detection_confidence=0.5, model_selection=0)\n",
      " |  \n",
      " |  MediaPipe Face Detection.\n",
      " |  \n",
      " |  MediaPipe Face Detection processes an RGB image and returns a list of the\n",
      " |  detected face location data.\n",
      " |  \n",
      " |  Please refer to\n",
      " |  https://solutions.mediapipe.dev/face_detection#python-solution-api\n",
      " |  for usage examples.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FaceDetection\n",
      " |      mediapipe.python.solution_base.SolutionBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, min_detection_confidence=0.5, model_selection=0)\n",
      " |      Initializes a MediaPipe Face Detection object.\n",
      " |      \n",
      " |      Args:\n",
      " |        min_detection_confidence: Minimum confidence value ([0.0, 1.0]) for face\n",
      " |          detection to be considered successful. See details in\n",
      " |          https://solutions.mediapipe.dev/face_detection#min_detection_confidence.\n",
      " |        model_selection: 0 or 1. 0 to select a short-range model that works\n",
      " |          best for faces within 2 meters from the camera, and 1 for a full-range\n",
      " |          model best for faces within 5 meters. See details in\n",
      " |          https://solutions.mediapipe.dev/face_detection#model_selection.\n",
      " |  \n",
      " |  process(self, image: numpy.ndarray) -> <class 'NamedTuple'>\n",
      " |      Processes an RGB image and returns a list of the detected face location data.\n",
      " |      \n",
      " |      Args:\n",
      " |        image: An RGB image represented as a numpy ndarray.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If the underlying graph throws any error.\n",
      " |        ValueError: If the input image is not three channel RGB.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A NamedTuple object with a \"detections\" field that contains a list of the\n",
      " |        detected face location data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mediapipe.python.solution_base.SolutionBase:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      A \"with\" statement support.\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_val, exc_tb)\n",
      " |      Closes all the input sources and the graph.\n",
      " |  \n",
      " |  close(self) -> None\n",
      " |      Closes all the input sources and the graph.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from mediapipe.python.solution_base.SolutionBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "# MediaPipe solution API\n",
    "mp_face_detection = mp.solutions.face_detection # creating a face detection object of mediapipe solution API\n",
    "\n",
    "help(mp_face_detection.FaceDetection)  # Documentation of the face detection object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DrawingSpec for drawing the face landmarks later.\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detections of short_length\\1.jpg:\n",
      "Face detections of short_length\\2.jpg:\n"
     ]
    }
   ],
   "source": [
    "# Run MediaPipe Face Detection with short range model.\n",
    "\n",
    "with mp_face_detection.FaceDetection(min_detection_confidence=0.5,model_selection=0) as face_detection:\n",
    "    for name, image in dic_short.items():\n",
    "        # Convert the BGR image to RGB and process it with MediaPipe Face Detection.\n",
    "        results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Draw face detections of each face.\n",
    "        print(f'Face detections of {name}:')\n",
    "        if not results.detections:\n",
    "            continue\n",
    "        annotated_image = image.copy()\n",
    "        for detection in results.detections:\n",
    "            mp_drawing.draw_detection(annotated_image, detection)\n",
    "            resize_and_show(name,annotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detections of long_length\\2.jpg:\n",
      "Face detections of long_length\\3.jpg:\n"
     ]
    }
   ],
   "source": [
    "# Run MediaPipe Face Detection with full range model.\n",
    "\n",
    "with mp_face_detection.FaceDetection(min_detection_confidence=0.5,model_selection=1) as face_detection:\n",
    "    for name, image in dic_long.items():\n",
    "        # Convert the BGR image to RGB and process it with MediaPipe Face Detection.\n",
    "        results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Draw face detections of each face.\n",
    "        print(f'Face detections of {name}:')\n",
    "        if not results.detections:\n",
    "            continue\n",
    "        annotated_image = image.copy()\n",
    "        for detection in results.detections:\n",
    "            mp_drawing.draw_detection(annotated_image, detection)\n",
    "        resize_and_show(name,annotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
